{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 528\n"
     ]
    }
   ],
   "source": [
    "page = wikipedia.page(\"Iceland\")\n",
    "text = page.content\n",
    "text = text.replace('=','')\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(f'Sentences: {len(sentences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The - DT\n",
      "capital - NN\n",
      "and - CC\n",
      "largest - JJS\n",
      "city - NN\n",
      "is - VBZ\n",
      "Reykjavík - NNP\n",
      ", - ,\n",
      "with - IN\n",
      "Reykjavík - NNP\n",
      "and - CC\n",
      "the - DT\n",
      "surrounding - VBG\n",
      "areas - NNS\n",
      "in - IN\n",
      "the - DT\n",
      "southwest - NN\n",
      "of - IN\n",
      "the - DT\n",
      "country - NN\n",
      "being - VBG\n",
      "home - VBN\n",
      "to - TO\n",
      "over - IN\n",
      "two-thirds - NNS\n",
      "of - IN\n",
      "the - DT\n",
      "population - NN\n",
      ". - .\n"
     ]
    }
   ],
   "source": [
    "tokens = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "tagged = [nltk.pos_tag(sent) for sent in tokens]\n",
    "\n",
    "for item in tagged[1]:\n",
    "    print(f'{item[0]} - {item[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (ne_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iceland - GPE\n",
      "Ísland - PERSON\n",
      "Nordic - GPE\n",
      "North Atlantic - LOCATION\n",
      "Europe - GPE\n",
      "Reykjavík - GPE\n",
      "Gulf Stream - ORGANIZATION\n",
      "Arctic Circle - ORGANIZATION\n",
      "Landnámabók - PERSON\n",
      "Norwegian - GPE\n",
      "Ingólfr Arnarson - PERSON\n",
      "Norwegians - GPE\n",
      "Gaelic - ORGANIZATION\n",
      "Kalmar Union - ORGANIZATION\n",
      "Norway - GPE\n",
      "Denmark - PERSON\n",
      "Sweden - GPE\n",
      "Danish - GPE\n",
      "Lutheranism - GPE\n",
      "French - GPE\n",
      "Napoleonic Wars - ORGANIZATION\n"
     ]
    }
   ],
   "source": [
    "tokens_ner = nltk.word_tokenize(text)\n",
    "tagged_ner = nltk.pos_tag(tokens_ner)\n",
    "ne_chunked = nltk.ne_chunk(tagged_ner)\n",
    "\n",
    "named_entities = {}\n",
    "for entity in ne_chunked:\n",
    "    if isinstance(entity, nltk.tree.Tree):\n",
    "        tmp = \" \".join([word for word, tag in entity.leaves()])\n",
    "        ent = entity.label()\n",
    "        named_entities[tmp] = ent\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "limit = 0\n",
    "tmp = []\n",
    "for key, value in named_entities.items():\n",
    "    if key in tmp:\n",
    "        continue\n",
    "    else:\n",
    "        tmp.append(key)\n",
    "    print(f'{key} - {value}')\n",
    "    if limit == 20:\n",
    "        break\n",
    "    limit +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper Secondary School Act\n",
      "Danish\n",
      "Phoca vitulina\n",
      "Index of Iceland-related\n",
      "Guinness World Records\n",
      "Hringvegur\n",
      "Sport Sport\n",
      "Icelandic Sign Language\n",
      "English\n",
      "Eurasian\n",
      "European\n",
      "Nesjavellir\n",
      "Guardian\n",
      "Fjallagrasa\n",
      "UN\n",
      "Iceland Plateau\n",
      "Academy Award for Best Foreign Language Film\n",
      "November\n",
      "Garðar Svavarsson\n",
      "Troll Peninsula in Northern Iceland\n"
     ]
    }
   ],
   "source": [
    "tokens_ner = nltk.word_tokenize(text)\n",
    "tagged_ner = nltk.pos_tag(tokens_ner)\n",
    "entities = []\n",
    "\n",
    "for sentence in tagged:\n",
    "    entity = []\n",
    "    for tagged_entry in sentence:\n",
    "\n",
    "        if tagged_entry[1].startswith(\"NNP\") or (entity and tagged_entry[1].startswith(\"IN\")):\n",
    "            entity.append(tagged_entry)\n",
    "        else:\n",
    "            if entity and entity[-1][1].startswith(\"IN\"):\n",
    "                entity.pop()\n",
    "            if(entity and \" \".join(e[0] for e in entity)[0].isupper()):\n",
    "                entities.append(\" \".join(e[0] for e in entity))\n",
    "            \n",
    "            entity = []\n",
    "\n",
    "\n",
    "my_set = set(entities)\n",
    "for i in list(my_set)[:20]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_classification(entity):\n",
    "    try:\n",
    "        page = wikipedia.page(entity)\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        return 'something'\n",
    "    sentences = nltk.sent_tokenize(page.summary)\n",
    "    tagged_tokens = nltk.pos_tag(nltk.word_tokenize(sentences[0]))\n",
    "    is_word = False\n",
    "    res = []\n",
    "    for word in tagged_tokens:\n",
    "        if word[1].startswith('VB'):\n",
    "            is_word = True\n",
    "        \n",
    "        if is_word:\n",
    "            if word[1].startswith('JJ') or word[1].startswith('NN'):\n",
    "                res.append(word[0])\n",
    "            else:\n",
    "                if res:\n",
    "                    return ' '.join(res)\n",
    "                else:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iceland = Nordic island country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pleton\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Pleton\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Icelandic = something\n",
      "North Atlantic = second largest\n",
      "Europe = continent\n",
      "Reykjavík = capital\n",
      "Gulf Stream = warm\n",
      "Arctic Circle = polar circles\n",
      "Landnámabók = Landnáma\n",
      "Ingólfr Arnarson = first permanent Norse settlers\n",
      "Norwegians = North Germanic ethnic group native\n",
      "Scandinavians = people\n",
      "Gaelic = something\n",
      "Althing = Althingi\n",
      "Kalmar Union = personal union\n",
      "Norway = Nordic country\n",
      "Denmark = [ ˈdanmɑɡ ]\n",
      "Sweden = Kingdom\n",
      "Danish = something\n",
      "Lutheranism = major branch\n",
      "Revolution = fundamental\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "tmp = []\n",
    "for entity in entities:\n",
    "    if entity in tmp:\n",
    "        continue\n",
    "    else:\n",
    "        tmp.append(entity)\n",
    "    if count <20:\n",
    "        count+=1\n",
    "    else:\n",
    "        break\n",
    "    res = get_wiki_classification(entity)\n",
    "    print(f'{entity} = {res}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
